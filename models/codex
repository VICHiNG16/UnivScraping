Below is a detailed review based on the current code and the available repository contents.
1. Code Review (Phase 8/9)
execution/scrapers/ucv/pdf_parser.py

    Hyphenation fix (-\n → space): I do not see an explicit hyphenation merge step in the current parser. The text extraction path builds raw_pages and passes them through BoilerplateRejector + Unicode normalization, but there is no direct replacement of "-\n" or similar hyphenation repair in _extract_via_text or _extract_via_tables. If you intended a hard hyphenation fix, it is not present in this file as it stands.

    Page-by-page logic: The table strategy iterates pages (for page in pdf.pages) and extracts tables per page, which is sound for page-by-page extraction. The text strategy concatenates/cleans the full text output from all pages (raw_pages → clean_text), which is acceptable but not strictly per-page after cleaning. This could make page-local context (like headers/footers) harder to attribute to specific rows, though it’s fine for global regex scanning.

execution/enrichment/matcher.py

    Conflict Modeling (Evidence > PDF Ranker): Evidence is appended per match (evidence["spots"]), and arbitration uses a combined score: likelihood_score + (score * 10) to choose the best source and update best_source_score metadata. This effectively prioritizes higher-truth documents when their content score is higher and the fuzzy match is strong, but it does not explicitly prefer “Evidence” over the ranker—evidence is just accumulated, and the arbitration logic still uses ranker-derived likelihood_score plus match score. So “Evidence > PDF Ranker” is not a strict priority; it is Evidence accumulation + ranker-weighted arbitration.

    Evidence includes page numbers? The evidence entry includes source, source_name, value, score, match_score, and timestamp — no page numbers are recorded here. If you want page-level traceability, it isn’t captured in the current evidence schema in matcher.py.

2. Data Quality Analysis (samples/ace/*.json)

I was unable to locate the samples/ace/ directory or any sample JSON files in this repo, so I could not inspect actual outputs.
What I looked for

    samples/ace/*.json (directory did not exist)

    Any JSON files with “ace” in the filename (none found)

Impact

Because these sample files are missing, I could not verify:

    Whether evidence contains PDF page numbers for programs like “Calculatoare”

    Whether spots_raw explains mismatches

    Whether “Garbage” entries (e.g., “Meniu”, “Secretariat”) slipped through

If you can point me to the actual output directory (or add the samples folder), I can review the data quality as requested.
Commands run

    sed -n '1,240p' execution/scrapers/ucv/pdf_parser.py

    sed -n '240,520p' execution/scrapers/ucv/pdf_parser.py

    rg -n "evidence|best_source_score|match_score|pdf_rank|rank" execution/enrichment/matcher.py

    ls samples/ace

    rg --files -g 'samples/**/*.json'

    ls samples

    rg --files -g '*ace*.json'

Summary of Findings (Short)

    Hyphenation fix is not present in pdf_parser.py (no -\n replacement).

    Page-by-page parsing exists in table extraction but text extraction is aggregated across all pages, which may limit page-level evidence linkage.

    Conflict Modeling uses ranker score + match score for arbitration; evidence is recorded but not given priority as a separate rule.

    Evidence entries do not include page numbers at this time.

    No samples found, so output verification could not be performed.

If you can supply the correct output path (or re-run to generate samples/ace/), I can validate evidence, spots_raw, and any garbage slip-throughs directly.

diff --git a/execution/enrichment/matcher.py b/execution/enrichment/matcher.py
index dc17a64020252fb13bb634bee56e9c95f357f9c9..337a4029159bed5cf21ffded4dfe58a460b97458 100644
--- a/execution/enrichment/matcher.py
+++ b/execution/enrichment/matcher.py
@@ -1,25 +1,26 @@
+import hashlib
 import json
 import logging
 from pathlib import Path
 from typing import List, Dict, Any, Optional
 import re
 from rapidfuzz import fuzz
 import datetime
 from execution.scrapers.ucv.pdf_parser import PDFParser
 
 logger = logging.getLogger("matcher")
 
 class RomanianProgramMatcher:
     """
     V4 Matching Engine for Romanian Academic Programs
     Uses multi-signal fusion: Name + Level + Domain + Code
     """
     def __init__(self, html_programs: List[Dict], pdf_rows: List[Dict]):
         self.html_programs = html_programs
         self.pdf_rows = pdf_rows
         # Pre-compile regex for performance
         self.abbrevs = {
             r'\bcalc\b': 'calculatoare',
             r'\beng\b': 'engleza',
             r'\bing\b': 'inginerie',
             r'\bauto\b': 'automatica',
@@ -318,59 +319,50 @@ class DataFusionEngine:
                     # V7: Post-Extraction Validation
                     valid_rows = [r for r in raw_rows if len(r['program_name']) > 5 and "copie" not in r['program_name'].lower()]
                     
                     if not valid_rows or len(valid_rows) < len(raw_rows) * 0.5:
                          logger.warning(f"[{slug}] PDF yielded mostly garbage rows (e.g. '{raw_rows[0].get('program_name')}'). Discarding.")
                          continue # Try next candidate
                     
                     pdf_rows_list.append({
                         "rows": valid_rows,
                         "url": candidate["pdf_url"],
                         "score": c_score,
                         "link_text": candidate["link_text"]
                     })
                     logger.info(f"[{slug}] Success! Extracted {len(valid_rows)} rows from {candidate['link_text']}")
                     # V8.7: Do NOT break. Continue to next candidate.
                     # break 
                 else:
                      logger.warning(f"[{slug}] Extraction failed (0 rows) for {pdf_path.name}. Trying next candidate...")
             except Exception as e:
                 logger.error(f"Error parsing {pdf_path}: {e}")
 
         if not pdf_rows_list:
             logger.warning(f"[{slug}] All PDF candidates failed to yield Data.")
             return
 
-        # V7: Post-Extraction Validation
-        # If all extracted rows are garbage (e.g. "Signature"), discard match and warn.
-        valid_rows = [r for r in pdf_rows if len(r['program_name']) > 5 and "copie" not in r['program_name'].lower()]
-        
-        if len(valid_rows) < len(pdf_rows) * 0.5:
-             logger.warning(f"[{slug}] PDF yielded mostly garbage rows (e.g. '{pdf_rows[0]['program_name']}'). Discarding.")
-             # Ideally we would loop back and try next candidate, but for now just filter?
-             pass # Logic flaw in previous step, fixed below.
-
         # 4. Phase 8.6: Load and Fuse Grades
         grades_map = self._load_grades(slug)
         
         # 3. Match and Update (Phase 8.7: Multi-PDF)
         # Iterate ALL valid candidates and fuse
         for res in pdf_rows_list:
              self._fuse_data(slug, programs, res["rows"], res["url"], faculty_uid, grades_map, res["score"], res["link_text"])
         
     def _load_grades(self, slug: str) -> Dict[str, float]:
         """
         Loads the most recent grades_map JSON from raw/slug/grades.
         """
         grades_dir = self.base_dir / "raw" / slug / "grades"
         if not grades_dir.exists(): return {}
         
         # Find latest file
         files = list(grades_dir.glob("grades_map_*.json"))
         if not files: return {}
         
         latest = max(files, key=lambda f: f.stat().st_mtime)
         try:
             with open(latest, "r", encoding="utf-8") as f:
                 return json.load(f)
         except Exception:
             return {}
@@ -379,99 +371,106 @@ class DataFusionEngine:
         """
         Identifies the best PDF candidates for "Unknown Spots" using the PDFTruthRanker.
         V8: Evidence-Driven Ranking (Replaces V5.1 Heuristics)
         """
         from execution.enrichment.pdf_ranker import PDFTruthRanker
         
         # Filter: Only valid links
         candidates = [p for p in pdf_queue if p.get("link_text") and (p.get("pdf_url") or p.get("url"))]
         
         # Ranker
         ranker = PDFTruthRanker(admission_year=2026) # TODO: Make Configurable
         ranked_candidates = ranker.rank_candidates(candidates)
         
         # Log top picks
         if ranked_candidates:
             top = ranked_candidates[0]
             logger.info(f"Top PDF Candidate: '{top.get('link_text')}' (Score: {top.get('stage_a_score')})")
             
         return ranked_candidates
 
     def _fuse_data(self, slug: str, programs: List[Dict], pdf_rows: List[Dict], pdf_url: str, faculty_uid: str, grades_map: Dict = None, likelihood_score: float = 0, source_name: str = ""):
         """
         Fuzzy matching logic using V4 Matcher.
         V8: Now accepting explicit faculty_uid (Hash) and slug.
         """
-        # V6: PDF-First Synthesis
-        if not programs and pdf_rows:
-            logger.info(f"[{slug}] PDF-Only Mode: Synthesizing {len(pdf_rows)} programs from PDF.")
-            for row in pdf_rows:
-                # V9: Zero Garbage Validation
-                val_res = self.validator.validate_program_name(row['program_name'])
-                if val_res["status"] == "FAIL":
-                    logger.warning(f"[{slug}] Dropping Garbage Candidate: '{row['program_name']}' (Reason: {val_res['reason']})")
-                    continue
-                if val_res["status"] == "QUARANTINE":
-                     logger.warning(f"[{slug}] Quarantining Candidate: '{row['program_name']}' (Score: {val_res['score']})")
-                     self._save_quarantine(slug, row, val_res)
-                     continue
+        # V9: Zero Garbage Validation (apply to all PDF rows)
+        filtered_rows = []
+        for row in pdf_rows:
+            val_res = self.validator.validate_program_name(row.get("program_name", ""))
+            if val_res["status"] == "FAIL":
+                logger.warning(f"[{slug}] Dropping Garbage Candidate: '{row.get('program_name')}' (Reason: {val_res['reason']})")
+                continue
+            if val_res["status"] == "QUARANTINE":
+                logger.warning(f"[{slug}] Quarantining Candidate: '{row.get('program_name')}' (Score: {val_res['score']})")
+                self._save_quarantine(slug, row, val_res)
+                continue
+            filtered_rows.append(row)
+
+        if not filtered_rows:
+            logger.warning(f"[{slug}] All PDF rows failed validation for source '{source_name}'.")
+            return
 
+        # V6: PDF-First Synthesis
+        if not programs and filtered_rows:
+            logger.info(f"[{slug}] PDF-Only Mode: Synthesizing {len(filtered_rows)} programs from PDF.")
+            for row in filtered_rows:
                 # Create Minimal Program Entity
                 match_id = hashlib.sha256(f"{slug}|{row['program_name']}".encode()).hexdigest()
                 career_paths = self._infer_career_paths(row['program_name'])
                 prog = {
                     "uid": match_id,
                     "name": row['program_name'],
                     "spots_budget": row['spots_budget'],
                     "spots_tax": row['spots_tax'],
                     "language": "ro", # Default
                     "level": row.get("level", "Master"), # V8: Use Detected Level or Default
                     "entity_type": "program",
                     "source_type": "pdf_only",
                     "faculty_uid": faculty_uid, # V8: True UID
                     "faculty_slug": slug,      # V8: Convenience Slug
                     "run_id": self.run_id,
                     "run_id": self.run_id,
                     "scraped_at": datetime.datetime.now(datetime.timezone.utc).isoformat(),
                     "source_url": pdf_url,
                     "text_for_embedding": (
                         f"Programul: {row['program_name']} ({row.get('level', 'Master')})\n"
                         f"Facultate: {slug.upper()}\n"
                         f"Locuri Buget: {row['spots_budget']} (Sursa: PDF {pdf_url})\n"
                         f"Locuri Taxa: {row['spots_tax']} (Sursa: PDF {pdf_url})\n"
                         f"Admitere: Media Licență\n"
                         f"[INFERENCE]\n"
                         f"Cariera: {', '.join(career_paths)}"
                     ),
                     "metadata": {"original_pdf": pdf_url},
                     "career_paths": career_paths,
                     "admission_year": self.admission_year
                 }
                 self._save_program(slug, prog)
             return
 
-        matcher = RomanianProgramMatcher(programs, pdf_rows)
+        matcher = RomanianProgramMatcher(programs, filtered_rows)
         results = matcher.match_all()
         
         for res in results:
             prog = res["program"]
             match = res["match"]
             score = res["score"]
             status = res["status"]
             
             html_name = prog["name"]
             
             # V8: Ensure IDs are consistent even for HTML programs
             prog["faculty_uid"] = faculty_uid
             prog["faculty_slug"] = slug
             
             if match and score > 0.65: # New threshold for weighted score
                 match_name = match["program_name"]
                 logger.info(f"Match ({status}): '{html_name}' <-> '{match_name}' ({score:.2f})")
                 
                 if status == "ambiguous":
                     logger.warning(f"  [AMBIGUOUS] Check manually: {html_name}")
                 
                 # UPDATE PROGRAM
                 # V8.7: Evidence Collection
                 prog["evidence"] = prog.get("evidence", {})
                 prog["evidence"]["spots"] = prog["evidence"].get("spots", [])


execution/models/program.py

diff --git a/execution/models/program.py b/execution/models/program.py
index 89c46ede6f560ccf1fed7db576d2ea2f87f1383d..a87e5c2d6f4393ba9cfa9eeda8f19a04857f7f74 100644
--- a/execution/models/program.py
+++ b/execution/models/program.py
@@ -1,41 +1,44 @@
-from typing import Optional
+from typing import Any, Optional
 from pydantic import Field
 from execution.models.base import BaseEntity
 
 class Program(BaseEntity):
     """
     Represents a Study Program (Specialization).
     """
     name: str = Field(..., description="Official name of the specialization.")
     faculty_uid: str = Field(..., description="Hashed UID of the parent faculty entity.")
     faculty_slug: str = Field(..., description="URL-friendly slug of the parent faculty (e.g. 'ace').")
     level: str = Field(..., description="Bachelor (Licenta), Master, PhD.")
     duration_years: Optional[str] = Field(None, description="Duration (e.g. '4 ani'). Stored as string to capture variants.")
     # V4 RAG Fields
     program_id: Optional[str] = Field(None, description="Stable ID for Vector DB.")
     language: Optional[str] = Field("Romanian", description="Language of instruction (normalized).")
     keywords: Optional[list[str]] = Field(default_factory=list, description="Search synonyms.")
     text_for_embedding: Optional[str] = Field(None, description="Narrative description for embedding.")
     admission_year: Optional[int] = Field(None, description="Year of admission stats.")
     
     # Admission stats (often from PDFs)
     spots_budget: Optional[int] = Field(None, description="Number of budget (free) spots.")
     spots_tax: Optional[int] = Field(None, description="Number of tuition-based spots.")
     
     # V3.2 Data Lineage fields
     spots_raw: Optional[str] = Field(None, description="Raw text for spots (e.g. '30 locuri') to prevent early validation loss.")
     source_type: str = Field("html", description="Source of data: 'html' (faculty page) or 'pdf' (admission doc).")
     accuracy_confidence: float = Field(0.5, description="0.0-1.0 score. HTML spots = low confidence. PDF table = high.")
     
     last_admission_grade: Optional[float] = Field(None, description="Last admission grade from previous year.")
     
     # Preservation of original raw text for debugging
     raw_admission_text: Optional[str] = Field(None, description="Raw string extracted for admission info (e.g. '30 locuri buget').")
 
     # V8.7 Conflict Modeling
-    evidence: Optional[dict] = Field(default_factory=dict, description="Stores conflicting data points from multiple sources. Keys: 'spots', 'grades'.")
+    evidence: Optional[dict[str, list[dict[str, Any]]]] = Field(
+        default_factory=dict,
+        description="Stores evidence arrays per data source (e.g. 'spots', 'grades'). Each entry can include source, scores, and timestamps."
+    )
 
     def __init__(self, **data):
         # Force entity_type to 'program'
         data['entity_type'] = 'program'
         super().__init__(**data)


execution/processors/validator.py


diff --git a/execution/processors/validator.py b/execution/processors/validator.py
index a17275cd4b77d8256316eb5508e8a54fdac2fd88..b9e6005fca960e7f7cb24f10d53ffd081f34e3d6 100644
--- a/execution/processors/validator.py
+++ b/execution/processors/validator.py
@@ -1,96 +1,123 @@
 
-import re
 import math
+import re
 from typing import Dict, Any, List
 
 class SemanticValidator:
     """
     Iron Dome V2: Heuristic Semantic Validation for Program Names.
     Filters out 'Garbage' (headers, navigation, random text) from 'Real Programs'.
     """
     
     def __init__(self):
         self.NEGATIVE_KEYWORDS = [
-            "secretariat", "contact", "acasa", "home", "meniu", "search", 
-            "regulament", "concurs", "biblioteca", "campus", "cazare", 
-            "burse", "orar", "proiecte", "parteneri", "despre", "istoric", 
-            "conducere", "departamente", "login", "harta", "gdpr", "cookies",
-            "anunturi", "evenimente", "noutati", "presă", "media", "galerie"
+            "secretariat", "contact", "acasa", "home", "meniu", "search",
+            "regulament", "concurs", "bibliotec", "campus", "cazare",
+            "burse", "orar", "proiect", "partener", "despre", "istoric",
+            "conducer", "departament", "login", "harta", "gdpr", "cookies",
+            "anunt", "eveniment", "noutat", "presa", "media", "galerie",
+            "admitere", "inscrier", "secretari"
         ]
         
         self.POSITIVE_KEYWORDS = [
-            "inginer", "stiint", "limb", "literatur", "studi", 
+            "inginer", "stiint", "limb", "literatur", "studi",
             "master", "licent", "manag", "drept", "informat", "tehnolog",
-            "matemat", "chimi", "fizic", "biolog", "geografi", 
             "matemat", "chimi", "fizic", "biolog", "geografi",
-            "istori", "teolog", "art", "muzic", "teatr", "pedagog",
-            "sport", "educati", "administra", "econom", "finant", "didac"
+            "istori", "teolog", "arte", "muzic", "teatr", "pedagog",
+            "sport", "educati", "administra", "econom", "finant", "didac",
+            "psiholog", "comunic", "sociolog", "arhitect", "construct",
+            "electr", "mecanic", "agronom", "horticult", "silvicult",
+            "marketing", "contab", "statistic", "kinetoterap", "farmac"
         ]
+
+        self.PROGRAM_SUFFIXES = [
+            "ologie", "istica", "logie", "grafie", "metrie", "nomic", "genie",
+            "turism", "silvic", "sanitar", "juridic"
+        ]
+
+    def _normalize_text(self, text: str) -> str:
+        text = text.lower().strip()
+        text = text.replace("ş", "s").replace("ș", "s").replace("ţ", "t").replace("ț", "t")
+        text = text.replace("ă", "a").replace("â", "a").replace("î", "i")
+        return re.sub(r"[^\w\s]", " ", text)
+
+    def _tokenize(self, text: str) -> List[str]:
+        return [t for t in text.split() if t]
+
+    def _has_keyword(self, tokens: List[str], keywords: List[str]) -> bool:
+        return any(any(token.startswith(kw) for token in tokens) for kw in keywords)
         
     def validate_program_name(self, name: str) -> Dict[str, Any]:
         """
         Validates a candidate program name.
         Returns: {
             "status": "PASS" | "FAIL" | "QUARANTINE",
             "score": float (0-100),
             "reason": str
         }
         """
         if not name:
             return {"status": "FAIL", "score": 0, "reason": "Empty string"}
             
-        name_lower = name.lower().strip()
+        name_norm = self._normalize_text(name)
+        tokens = self._tokenize(name_norm)
         
         # 1. Entropy / Stucture Checks
         if len(name) < 4:
             return {"status": "FAIL", "score": 0, "reason": "Too short"}
         
         if len(name) > 150:
              return {"status": "FAIL", "score": 0, "reason": "Too long"}
              
         # Digit Ratio (Programs shouldn't be mostly numbers)
-        digit_count = sum(c.isdigit() for c in name)
+        digit_count = sum(c.isdigit() for c in name_norm)
         if digit_count / len(name) > 0.4:
              return {"status": "FAIL", "score": 10, "reason": "High digit ratio (looks like phone/CNP)"}
 
+        # Entropy / diversity check for noisy strings like "aaaaa" or repeated symbols
+        alpha_chars = [c for c in name_norm if c.isalpha()]
+        unique_alpha = len(set(alpha_chars))
+        if alpha_chars and len(alpha_chars) >= 6 and unique_alpha <= 2:
+            return {"status": "FAIL", "score": 5, "reason": "Low character diversity"}
+
         # 2. Negative Keywords (Iron Dome)
         for kw in self.NEGATIVE_KEYWORDS:
-            if kw in name_lower:
-                # "Secretariat" is fatal. "Contact" is fatal.
-                # Check for context: "Secretariatul Facultatii" vs "Secretariat si Administratie"
+            if any(token.startswith(kw) for token in tokens):
                 return {"status": "FAIL", "score": 0, "reason": f"Negative keyword: {kw}"}
                 
         # 3. Positive Keywords (Boost)
         pos_score = 0
         for kw in self.POSITIVE_KEYWORDS:
-            if kw in name_lower:
+            if any(token.startswith(kw) for token in tokens):
                 pos_score += 20
-                # If name IS the keyword (nearly), extra boost
-                # e.g. "Informatica" vs "Facultatea de Informatica"
-                if len(name_lower) <= len(kw) + 3:
-                     pos_score += 15
+                if len(name_norm) <= len(kw) + 3:
+                    pos_score += 15
+
+        # Root/Suffix heuristic (Romanian program morphology)
+        if any(any(token.endswith(suffix) for suffix in self.PROGRAM_SUFFIXES) for token in tokens):
+            pos_score += 10
         
         # 4. Heuristic Scoring
         # Base score starts low, needs positive signal
         score = 20 + pos_score
         
         # Word count check: Real programs usually have 2-8 words.
         # "Informatica" (1 word) is valid.
         # "Bun venit la facultatea noastra" (6 words) is invalid.
         words = name.split()
         if len(words) == 1 and score < 50:
              # Single word, no positive keyword? Risky.
              # e.g. "Diverse" -> FAIL
              pass
         elif len(words) >= 2:
             # Multi-word names are more likely to be real programs
             score += 10
         
         # Title Case Bonus
         # "Ingineria Sistemelor" vs "ingineria sistemelor" vs "CONTACT"
         if name.istitle():
             score += 10
             
         # Decision
         if score >= 45: # Lowered from 50
             return {"status": "PASS", "score": score, "reason": "Good score"}
